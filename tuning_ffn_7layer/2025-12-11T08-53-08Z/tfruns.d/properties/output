
> ############################################################
> # nn_experiment.R
> # Feed-forward neural network for Assignment 2
> # - full preproc .... [TRUNCATED] 

> ###############
> # Libraries
> ###############
> if (!require("dplyr"))      install.packages("dplyr");      library(dplyr)

> if (!require("caret"))      install.packages("caret");      library(caret)

> if (!require("keras3"))     install.packages("keras3");     library(keras3)

> if (!require("tensorflow")) install.packages("tensorflow"); library(tensorflow)

> if (!require("tfruns"))     install.packages("tfruns");     library(tfruns)

> if (!require("fastDummies")) install.packages("fastDummies"); library(fastDummies)

> ############################################################
> # 1. Data loading and preprocessing
> ############################################### .... [TRUNCATED] 

> dataset <- raw

> if ("ID" %in% names(dataset)) {
+   dataset <- dataset %>% dplyr::select(-ID)
+ }

> # Step 2: Replace empty strings in character columns with NA
> for (col in names(dataset)) {
+   if (is.character(dataset[[col]])) {
+     dataset[[ .... [TRUNCATED] 

> # Step 3: Convert target column 'status' to factor
> dataset$status <- as.factor(dataset$status)

> # Step 4: Handle special placeholder in DAYS_EMPLOYED (365243 = unemployed)
> dataset$is_unemployed <- ifelse(dataset$DAYS_EMPLOYED == 365243, 1, 0)

> dataset$DAYS_EMPLOYED[dataset$DAYS_EMPLOYED == 365243] <- NA

> # Step 5: Create age and years employed (in years)
> dataset$AGE            <- abs(dataset$DAYS_BIRTH) / 365

> dataset$YEARS_EMPLOYED <- abs(dataset$DAYS_EMPLOYED) / 365

> # Remove original day-based columns
> dataset <- dataset %>% dplyr::select(-DAYS_BIRTH, -DAYS_EMPLOYED)

> # Step 6: Treat missing OCCUPATION_TYPE as its own category
> dataset$OCCUPATION_TYPE[is.na(dataset$OCCUPATION_TYPE)] <- "Unknown"

> # Step 7: Cap extreme values in family members and children
> dataset$CNT_FAM_MEMBERS <- pmin(dataset$CNT_FAM_MEMBERS, 10)

> dataset$CNT_CHILDREN    <- pmin(dataset$CNT_CHILDREN, 6)

> # Step 8: Log-transform income to reduce skewness
> dataset$AMT_INCOME_TOTAL <- log1p(dataset$AMT_INCOME_TOTAL)

> # Step 9: Remove uninformative constant feature FLAG_MOBIL (always 1)
> if ("FLAG_MOBIL" %in% names(dataset)) {
+   dataset <- dataset %>% dplyr::se .... [TRUNCATED] 

> # Step 10: Separate predictors and target
> # features <- dataset %>% dplyr::select(-status)
> 
> # Step 11: Encode character predictors as numeric  .... [TRUNCATED] 

> # identify categorical columns (character or factor), except target
> cat_cols <- names(dataset)[sapply(dataset, is.character) | sapply(dataset, is. .... [TRUNCATED] 

> cat_cols <- setdiff(cat_cols, "status")

> # Step 11: Dummy encode categorical predictors (one-hot)
> dataset_dummies <- fastDummies::dummy_cols(
+   dataset,
+   select_columns = cat_cols,
+ .... [TRUNCATED] 

> # Step 12: Separate predictors and target again
> features <- dataset_dummies %>% dplyr::select(-status)

> # Step 13: Median imputation for remaining numeric NAs
> for (col in names(features)) {
+   if (is.numeric(features[[col]])) {
+     if (any(is.na(f .... [TRUNCATED] 

> # Step 14: Scale all numeric predictors to [0,1]
> scale_to_zero_one <- function(x) {
+   if (!is.numeric(x)) return(x)
+   if (all(is.na(x)))  retu .... [TRUNCATED] 

> features_scaled <- as.data.frame(lapply(features, scale_to_zero_one))

> features_scaled_num <- as.data.frame(lapply(features_scaled, as.numeric))

> # Final x and y
> x <- data.matrix(features_scaled_num)

> y_factor <- dataset_dummies$status

> y_num <- as.numeric(y_factor) - 1

> num_classes <- length(unique(y_num))

> ############################################################
> # 2. Train / validation / test split (stratified)
> ################################# .... [TRUNCATED] 

> train_index <- createDataPartition(y_num, p = 0.7, list = FALSE)

> x_train <- x[train_index, ]

> y_train <- y_num[train_index]

> x_temp <- x[-train_index, ]

> y_temp <- y_num[-train_index]

> set.seed(1)

> val_index <- createDataPartition(y_temp, p = 0.5, list = FALSE)

> x_val <- x_temp[val_index, ]

> y_val <- y_temp[val_index]

> x_test <- x_temp[-val_index, ]

> y_test <- y_temp[-val_index]

> ############################################################
> # 3. Class weights to handle imbalance
> ############################################ .... [TRUNCATED] 

> raw_w <- 1 / sqrt(freq)      # softer than 1/freq

> w <- raw_w / mean(raw_w)     # normalise around 1

> class_weights <- as.list(as.numeric(w))

> names(class_weights) <- names(freq)

> print(class_weights)
$`0`
[1] 0.1102361

$`1`
[1] 0.3121624

$`2`
[1] 0.9365901

$`3`
[1] 1.884386

$`4`
[1] 2.536298

$`5`
[1] 1.29665

$`6`
[1] 0.5911844

$`7`
[1] 0.3324927


> ############################################################
> # 4. Hyperparameters via FLAGS (for tfruns::tuning_run)
> ########################### .... [TRUNCATED] 
